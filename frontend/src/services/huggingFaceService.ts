// src/services/huggingFaceService.ts
export interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface RetryConfig {
  maxRetries: number;
  baseDelay: number;
  maxDelay: number;
}

interface ModelConfig {
  name: string;
  maxTokens: number;
  temperature: number;
  topP: number;
}

class HuggingFaceService {
  private baseURL = 'https://api-inference.huggingface.co/models';
  private apiKey: string;
  private models: ModelConfig[] = [
    {
      name: 'microsoft/DialoGPT-large',
      maxTokens: 150,
      temperature: 0.7,
      topP: 0.9
    },
    {
      name: 'microsoft/DialoGPT-medium',
      maxTokens: 120,
      temperature: 0.8,
      topP: 0.9
    },
    {
      name: 'gpt2',
      maxTokens: 100,
      temperature: 0.8,
      topP: 0.9
    },
    {
      name: 'google/flan-t5-base',
      maxTokens: 100,
      temperature: 0.7,
      topP: 0.95
    }
  ];
  
  private currentModelIndex = 0;
  private retryConfig: RetryConfig = {
    maxRetries: 3,
    baseDelay: 2000,
    maxDelay: 10000
  };

  constructor() {
    this.apiKey = this.getApiKey();
    
    console.log('üîç Initialisation Hugging Face Service:');
    console.log('API Key:', this.apiKey ? '‚úÖ TROUV√âE' : '‚ùå MANQUANTE');
    console.log('Mod√®les disponibles:', this.models.length);
  }

  private getApiKey(): string {
    try {
      // Essayer diff√©rentes sources pour la cl√© API
      const sources = [
        // Vite
        () => typeof import.meta !== 'undefined' && import.meta.env?.VITE_HUGGINGFACE_API_KEY,
        // Process env
        () => typeof process !== 'undefined' && process.env?.VITE_HUGGINGFACE_API_KEY,
        // Window global
        () => typeof window !== 'undefined' && (window as any).VITE_HUGGINGFACE_API_KEY,
        // Hardcoded fallback (√† √©viter en production)
        () => ''
      ];

      for (const getKey of sources) {
        const key = getKey();
        if (key) {
          console.log('‚úÖ Cl√© API trouv√©e');
          return key;
        }
      }

      console.warn('‚ö†Ô∏è Cl√© API Hugging Face non trouv√©e');
      return '';
    } catch (error) {
      console.error('‚ùå Erreur lors de la r√©cup√©ration de la cl√© API:', error);
      return '';
    }
  }

  private async sleep(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  private calculateBackoffDelay(attempt: number): number {
    const delay = this.retryConfig.baseDelay * Math.pow(2, attempt);
    const jitter = Math.random() * 1000; // Ajouter du jitter
    return Math.min(delay + jitter, this.retryConfig.maxDelay);
  }

  private buildConversationalPrompt(message: string, conversationHistory: ChatMessage[] = []): string {
    // Contexte syst√®me am√©lior√©
    let prompt = `Tu es un assistant IA intelligent et serviable. Tu r√©ponds de mani√®re concise, pr√©cise et utile en fran√ßais. 
Tu es amical mais professionnel. Si tu ne connais pas quelque chose, tu l'admets honn√™tement.

`;
    
    // Ajouter l'historique r√©cent (limit√© √† 4 √©changes pour √©viter les tokens excessifs)
    const recentHistory = conversationHistory.slice(-8); // 4 √©changes = 8 messages
    
    for (const msg of recentHistory) {
      if (msg.role === 'user') {
        prompt += `Utilisateur: ${msg.content}\n`;
      } else if (msg.role === 'assistant') {
        prompt += `Assistant: ${msg.content}\n`;
      }
    }
    
    // Ajouter le message actuel
    prompt += `Utilisateur: ${message}\nAssistant:`;
    
    return prompt;
  }

  private async makeRequest(modelConfig: ModelConfig, prompt: string, attempt: number = 0): Promise<any> {
    if (!this.apiKey) {
      throw new Error('API_KEY_MISSING');
    }

    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 45000); // Timeout augment√©

    try {
      console.log(`üöÄ Tentative avec ${modelConfig.name} (essai ${attempt + 1})`);

      const requestBody = {
        inputs: prompt,
        parameters: {
          max_new_tokens: modelConfig.maxTokens,
          temperature: modelConfig.temperature,
          top_p: modelConfig.topP,
          repetition_penalty: 1.1,
          return_full_text: false,
          do_sample: true,
          ...(modelConfig.name.includes('gpt') && { pad_token_id: 50256 }),
          ...(modelConfig.name.includes('flan-t5') && { 
            max_length: modelConfig.maxTokens + prompt.length 
          })
        },
        options: {
          wait_for_model: true,
          use_cache: false
        }
      };

      const response = await fetch(`${this.baseURL}/${modelConfig.name}`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.apiKey}`,
          'User-Agent': 'FileChat/1.0'
        },
        body: JSON.stringify(requestBody),
        signal: controller.signal
      });

      clearTimeout(timeoutId);

      // Succ√®s
      if (response.ok) {
        const data = await response.json();
        console.log(`‚úÖ R√©ponse re√ßue de ${modelConfig.name}`);
        return { data, modelUsed: modelConfig.name };
      }

      // Gestion des erreurs sp√©cifiques
      const errorText = await response.text().catch(() => 'Erreur inconnue');
      
      if (response.status === 503) {
        console.log(`‚è±Ô∏è Mod√®le ${modelConfig.name} en chargement...`);
        if (attempt < this.retryConfig.maxRetries) {
          const delay = this.calculateBackoffDelay(attempt);
          console.log(`üîÑ Retry dans ${delay}ms`);
          await this.sleep(delay);
          return this.makeRequest(modelConfig, prompt, attempt + 1);
        }
      }

      if (response.status === 429) {
        console.log(`‚è±Ô∏è Rate limit atteint pour ${modelConfig.name}`);
        if (attempt < this.retryConfig.maxRetries) {
          const delay = this.calculateBackoffDelay(attempt);
          await this.sleep(delay);
          return this.makeRequest(modelConfig, prompt, attempt + 1);
        }
      }

      throw new Error(`HTTP_${response.status}: ${errorText}`);

    } catch (error) {
      clearTimeout(timeoutId);
      
      if (error instanceof Error && error.name === 'AbortError') {
        throw new Error('TIMEOUT: La requ√™te a pris trop de temps');
      }

      // Retry pour les erreurs r√©seau
      if (attempt < this.retryConfig.maxRetries && (
        error instanceof TypeError || 
        (error instanceof Error && error.message.includes('fetch'))
      )) {
        const delay = this.calculateBackoffDelay(attempt);
        console.log(`üåê Retry r√©seau dans ${delay}ms`);
        await this.sleep(delay);
        return this.makeRequest(modelConfig, prompt, attempt + 1);
      }

      throw error;
    }
  }

  private async tryModelsSequentially(prompt: string): Promise<any> {
    let lastError: Error | null = null;

    // Essayer tous les mod√®les dans l'ordre
    for (let i = 0; i < this.models.length; i++) {
      const modelIndex = (this.currentModelIndex + i) % this.models.length;
      const model = this.models[modelIndex];

      try {
        const result = await this.makeRequest(model, prompt);
        // Si succ√®s, utiliser ce mod√®le comme priorit√© pour la prochaine fois
        this.currentModelIndex = modelIndex;
        return result;
      } catch (error) {
        console.warn(`‚ùå √âchec avec ${model.name}:`, error);
        lastError = error as Error;
        
        // Si ce n'est pas une erreur de disponibilit√©, passer au mod√®le suivant
        if (error instanceof Error && !error.message.includes('503')) {
          continue;
        }
        
        // Pour les erreurs 503, attendre un peu avant le prochain mod√®le
        if (i < this.models.length - 1) {
          await this.sleep(1000);
        }
      }
    }

    // Si tous les mod√®les ont √©chou√©
    throw lastError || new Error('Tous les mod√®les sont indisponibles');
  }

  private extractResponse(generatedText: string, inputPrompt: string, modelUsed: string): string {
    try {
      let response = generatedText;
      
      // Traitement sp√©cifique selon le mod√®le
      if (modelUsed.includes('flan-t5')) {
        // Flan-T5 retourne souvent des r√©ponses plus directes
        response = response.trim();
      } else {
        // Pour DialoGPT et GPT-2
        if (response.includes(inputPrompt)) {
          response = response.replace(inputPrompt, '').trim();
        }
        
        // Nettoyer les marqueurs de conversation
        response = response
          .split('\nUtilisateur:')[0]
          .split('\nUser:')[0]
          .split('\nHuman:')[0]
          .split('Utilisateur:')[0]
          .trim();
      }
      
      // Supprimer les pr√©fixes ind√©sirables
      response = response.replace(/^(Assistant:|Bot:|AI:)\s*/i, '');
      
      // G√©rer la longueur de la r√©ponse
      if (response.length > 300) {
        response = response.substring(0, 300);
        const lastSentence = Math.max(
          response.lastIndexOf('.'),
          response.lastIndexOf('!'),
          response.lastIndexOf('?')
        );
        
        if (lastSentence > response.length * 0.6) {
          response = response.substring(0, lastSentence + 1);
        } else {
          response += '...';
        }
      }
      
      // Nettoyer et valider
      response = response.trim();
      
      // V√©rifier la qualit√© de la r√©ponse
      if (!response || response.length < 3) {
        return 'Je suis d√©sol√©, je n\'ai pas pu g√©n√©rer une r√©ponse appropri√©e. Pouvez-vous reformuler votre question ?';
      }
      
      // V√©rifier si la r√©ponse n'est pas juste une r√©p√©tition
      const words = response.split(' ');
      if (words.length < 3) {
        return 'Pouvez-vous √™tre plus sp√©cifique dans votre question ? Je peux vous aider avec plus de d√©tails.';
      }
      
      return response;
      
    } catch (error) {
      console.error('‚ùå Erreur lors de l\'extraction:', error);
      return 'Erreur lors du traitement de la r√©ponse. Veuillez r√©essayer.';
    }
  }

  async generateResponse(message: string, conversationHistory: ChatMessage[] = []): Promise<string> {
    if (!this.apiKey) {
      return '‚ùå Configuration manquante : Cl√© API Hugging Face requise. V√©rifiez vos param√®tres.';
    }

    if (!message.trim()) {
      return 'Pouvez-vous poser une question ? Je suis l√† pour vous aider ! üòä';
    }

    try {
      console.log('ü§ñ G√©n√©ration de r√©ponse...');
      
      const prompt = this.buildConversationalPrompt(message, conversationHistory);
      console.log('üìù Prompt g√©n√©r√©:', prompt.substring(0, 200) + '...');
      
      const result = await this.tryModelsSequentially(prompt);
      const { data, modelUsed } = result;
      
      console.log('‚úÖ Donn√©es re√ßues:', JSON.stringify(data).substring(0, 200));
      
      // Extraire le texte g√©n√©r√©
      let generatedText = '';
      
      if (Array.isArray(data) && data.length > 0) {
        generatedText = data[0].generated_text || '';
      } else if (data && typeof data === 'object' && data.generated_text) {
        generatedText = data.generated_text;
      } else if (typeof data === 'string') {
        generatedText = data;
      }
      
      if (!generatedText) {
        console.error('‚ùå Pas de texte g√©n√©r√©:', data);
        return 'Je n\'ai pas pu g√©n√©rer une r√©ponse. R√©essayez dans quelques instants.';
      }
      
      const finalResponse = this.extractResponse(generatedText, prompt, modelUsed);
      console.log(`üéØ R√©ponse finale (${modelUsed}):`, finalResponse);
      
      return finalResponse;
      
    } catch (error) {
      console.error('‚ùå Erreur g√©n√©ration:', error);
      
      // Messages d'erreur user-friendly
      if (error instanceof Error) {
        if (error.message.includes('API_KEY_MISSING')) {
          return 'üîê Cl√© API manquante. Configurez votre cl√© Hugging Face dans les param√®tres.';
        } else if (error.message.includes('401')) {
          return 'üîê Cl√© API invalide. V√©rifiez votre cl√© Hugging Face dans les param√®tres.';
        } else if (error.message.includes('429')) {
          return '‚è±Ô∏è Trop de requ√™tes. Attendez quelques minutes avant de r√©essayer.';
        } else if (error.message.includes('503')) {
          return 'üîÑ Tous les mod√®les IA sont en cours de chargement. Attendez 1-2 minutes et r√©essayez.';
        } else if (error.message.includes('TIMEOUT')) {
          return '‚è±Ô∏è D√©lai d\'attente d√©pass√©. Le service est peut-√™tre surcharg√©, r√©essayez.';
        } else if (error.message.includes('fetch') || error instanceof TypeError) {
          return 'üåê Probl√®me de connexion. V√©rifiez votre connexion internet et r√©essayez.';
        }
      }
      
      return '‚ùå Une erreur inattendue s\'est produite. R√©essayez dans quelques instants.';
    }
  }

  async checkApiStatus(): Promise<{ status: 'ok' | 'error' | 'checking', message: string }> {
    if (!this.apiKey) {
      return { status: 'error', message: 'Cl√© API manquante' };
    }

    try {
      console.log('üîç V√©rification du statut API...');
      
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), 15000);

      // Tester avec le mod√®le le plus stable
      const testModel = this.models[2]; // gpt2
      
      const response = await fetch(`${this.baseURL}/${testModel.name}`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          inputs: 'Test',
          parameters: { max_new_tokens: 5, temperature: 0.7 },
          options: { wait_for_model: false }
        }),
        signal: controller.signal
      });

      clearTimeout(timeoutId);

      if (response.ok) {
        return { status: 'ok', message: 'API fonctionnelle' };
      } else if (response.status === 503) {
        return { status: 'ok', message: 'API disponible (mod√®les en chargement)' };
      } else if (response.status === 401) {
        return { status: 'error', message: 'Cl√© API invalide' };
      } else if (response.status === 429) {
        return { status: 'ok', message: 'API fonctionnelle (limite de taux)' };
      } else {
        return { status: 'error', message: `Erreur API: ${response.status}` };
      }
      
    } catch (error) {
      console.error('‚ùå Erreur v√©rification API:', error);
      
      if (error instanceof Error && error.name === 'AbortError') {
        return { status: 'error', message: 'D√©lai d\'attente d√©pass√©' };
      }
      return { status: 'error', message: 'Erreur de connexion' };
    }
  }

  // M√©thode pour obtenir des informations sur les mod√®les disponibles
  getAvailableModels(): ModelConfig[] {
    return [...this.models];
  }

  // M√©thode pour changer le mod√®le prioritaire
  setPreferredModel(modelName: string): boolean {
    const index = this.models.findIndex(m => m.name === modelName);
    if (index !== -1) {
      this.currentModelIndex = index;
      console.log(`‚úÖ Mod√®le prioritaire chang√© vers: ${modelName}`);
      return true;
    }
    return false;
  }
}

// Export d'une instance singleton
export const huggingfaceService = new HuggingFaceService();
export default huggingfaceService;